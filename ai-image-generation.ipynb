{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc9e1b0-5f69-456b-9a55-74e3eebaa7ba",
   "metadata": {},
   "source": [
    "### Packages(Dependency) installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa554ef-b866-4337-9e21-42b3720f8700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers\n",
      "  Using cached diffusers-0.34.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.16.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting safetensors\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting importlib_metadata (from diffusers)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting filelock (from diffusers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub>=0.27.0 (from diffusers)\n",
      "  Downloading huggingface_hub-0.33.2-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy (from diffusers)\n",
      "  Downloading numpy-2.3.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting regex!=2019.12.17 (from diffusers)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /media/heytanix/Shared/JupyterLabEnvironment/jupyter/lib/python3.13/site-packages (from diffusers) (2.32.4)\n",
      "Collecting Pillow (from diffusers)\n",
      "  Downloading pillow-11.3.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /media/heytanix/Shared/JupyterLabEnvironment/jupyter/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /media/heytanix/Shared/JupyterLabEnvironment/jupyter/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /media/heytanix/Shared/JupyterLabEnvironment/jupyter/lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Collecting torch>=2.0.0 (from accelerate)\n",
      "  Downloading torch-2.7.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.27.0->diffusers)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /media/heytanix/Shared/JupyterLabEnvironment/jupyter/lib/python3.13/site-packages (from huggingface-hub>=0.27.0->diffusers) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.27.0->diffusers)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: setuptools in /media/heytanix/Shared/JupyterLabEnvironment/jupyter/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.0.0->accelerate)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=2.0.0->accelerate)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /media/heytanix/Shared/JupyterLabEnvironment/jupyter/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch>=2.0.0->accelerate)\n",
      "  Downloading triton-3.3.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata->diffusers)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /media/heytanix/Shared/JupyterLabEnvironment/jupyter/lib/python3.13/site-packages (from requests->diffusers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /media/heytanix/Shared/JupyterLabEnvironment/jupyter/lib/python3.13/site-packages (from requests->diffusers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /media/heytanix/Shared/JupyterLabEnvironment/jupyter/lib/python3.13/site-packages (from requests->diffusers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /media/heytanix/Shared/JupyterLabEnvironment/jupyter/lib/python3.13/site-packages (from requests->diffusers) (2025.6.15)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.0.0->accelerate)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /media/heytanix/Shared/JupyterLabEnvironment/jupyter/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Using cached diffusers-0.34.0-py3-none-any.whl (3.8 MB)\n",
      "Using cached transformers-4.53.0-py3-none-any.whl (10.8 MB)\n",
      "Using cached accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "Downloading scipy-1.16.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/35.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
      "Downloading numpy-2.3.1-cp313-cp313-manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Downloading torch-2.7.1-cp313-cp313-manylinux_2_28_x86_64.whl (821.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.0/821.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:11\u001b[0mm\n",
      "\u001b[?25hUsing cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0mm\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading pillow-11.3.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m362.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, zipp, triton, tqdm, sympy, safetensors, regex, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, hf-xet, fsspec, filelock, scipy, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, importlib_metadata, huggingface-hub, tokenizers, nvidia-cusolver-cu12, diffusers, transformers, torch, accelerate\n",
      "Successfully installed Pillow-11.3.0 accelerate-1.8.1 diffusers-0.34.0 filelock-3.18.0 fsspec-2025.5.1 hf-xet-1.1.5 huggingface-hub-0.33.2 importlib_metadata-8.7.0 mpmath-1.3.0 networkx-3.5 numpy-2.3.1 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 regex-2024.11.6 safetensors-0.5.3 scipy-1.16.0 sympy-1.14.0 tokenizers-0.21.2 torch-2.7.1 tqdm-4.67.1 transformers-4.53.0 triton-3.3.1 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers transformers accelerate scipy safetensors --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4e2f95-7cd8-4cb0-954b-dc9710f59f40",
   "metadata": {},
   "source": [
    "### Packages Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb806245-ecea-4cc2-91b4-389f067c6c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bcb6c8-0976-4575-a403-c6b5c6ce05ad",
   "metadata": {},
   "source": [
    "### Checking GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "797b83ce-faee-4b32-85ec-07c4e2aedfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available True\n",
      "Using device: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "PyTorch version: 2.7.1+cu126\n",
      "GPU Memory Summary: |===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# For checking if GPU is available\n",
    "if not torch.cuda.is_available(): #If Torch discovers no GPU\n",
    "    raise RuntimeError(\"GPU Unavailable, Ensure PyTorch was installed with CUDA support.\")\n",
    "    #Raises a Runtime Error stating that the local device Has no CUDA enabled GPU\n",
    "\n",
    "print(\"CUDA Available\", torch.cuda.is_available()) #If CUDA is available prints \"CUDA Available\"\n",
    "print(\"Using device:\", torch.cuda.get_device_name()) #If CUDA is available prints the CUDA device being used\n",
    "print(\"PyTorch version:\", torch.__version__) #Prints the version of Torch being used\n",
    "print(\"GPU Memory Summary:\", torch.cuda.memory_summary()) #Prints the summary of the GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c1da4-b8de-4861-9577-99948f8dd204",
   "metadata": {},
   "source": [
    "### Choosing the Hardware (CPU/CUDA-GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d0033bb-4eeb-4306-8c65-3cdcf3d98fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0218fc-e19e-4edb-b7d6-d86eb0aedccf",
   "metadata": {},
   "source": [
    "### Using Hugging Face's runwayml/stable-diffusion-v1-5\n",
    "#### This will download the model the first time (about 4GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bf4cfa1-8134-407a-b1aa-a318e5d230f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bf17cced394f068629d258fd687776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb151e497b048ee9d5abf474fe61bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  49%|####9     | 1.18G/2.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109d0dba53754a3db4879af24d3a76e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:  23%|##2       | 1.00G/4.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/repos/66/6f/666f465fa70158515404e8de2c6bc6fe2f90c46f9296293aa14daededeb32c52/19da7aaa4b880e59d56843f1fcb4dd9b599c28a1d9d9af7c1143057c8ffae9f1?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27diffusion_pytorch_model.safetensors%3B+filename%3D%22diffusion_pytorch_model.safetensors%22%3B&Expires=1751445740&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTQ0NTc0MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy82Ni82Zi82NjZmNDY1ZmE3MDE1ODUxNTQwNGU4ZGUyYzZiYzZmZTJmOTBjNDZmOTI5NjI5M2FhMTRkYWVkZWRlYjMyYzUyLzE5ZGE3YWFhNGI4ODBlNTlkNTY4NDNmMWZjYjRkZDliNTk5YzI4YTFkOWQ5YWY3YzExNDMwNTdjOGZmYWU5ZjE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=qL-5w58vFsWoYLqRl7lg6mfBnGVcmQtFA9TWgUwnKpWnkvk%7ENeIO6vbsLQjXMeLb0LkyLhdwQ28RHXavx1ugbmpFFf5AYxrIIhvUFVSFC42Tdw9ypIOTwrn7C2k-rpDRRbLl%7EPPvtjFYFujwGLSpSCT5gnZp4nInbwrB8tzqiVxQkbBEA3dyRUO7N1%7EF0eh37naGrEjZRqu7S703ahODcF3DutIF9ep3KiYUnFYQjjhoRfJH03r2ubp4hPo9BpCcUjDxmpzE18Q6qtnUBfLxMiIyHrEq7VrCtkdB65TOHdAKomLXCFdmpSJEBOmZDi7BCAZ1d4%7EZSxF2Lq4wqFZwNg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4687ea6988da41edb11191b62759aa92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:  37%|###6      | 2.02G/5.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/repos/66/6f/666f465fa70158515404e8de2c6bc6fe2f90c46f9296293aa14daededeb32c52/19da7aaa4b880e59d56843f1fcb4dd9b599c28a1d9d9af7c1143057c8ffae9f1?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27diffusion_pytorch_model.safetensors%3B+filename%3D%22diffusion_pytorch_model.safetensors%22%3B&Expires=1751445740&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTQ0NTc0MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy82Ni82Zi82NjZmNDY1ZmE3MDE1ODUxNTQwNGU4ZGUyYzZiYzZmZTJmOTBjNDZmOTI5NjI5M2FhMTRkYWVkZWRlYjMyYzUyLzE5ZGE3YWFhNGI4ODBlNTlkNTY4NDNmMWZjYjRkZDliNTk5YzI4YTFkOWQ5YWY3YzExNDMwNTdjOGZmYWU5ZjE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=qL-5w58vFsWoYLqRl7lg6mfBnGVcmQtFA9TWgUwnKpWnkvk%7ENeIO6vbsLQjXMeLb0LkyLhdwQ28RHXavx1ugbmpFFf5AYxrIIhvUFVSFC42Tdw9ypIOTwrn7C2k-rpDRRbLl%7EPPvtjFYFujwGLSpSCT5gnZp4nInbwrB8tzqiVxQkbBEA3dyRUO7N1%7EF0eh37naGrEjZRqu7S703ahODcF3DutIF9ep3KiYUnFYQjjhoRfJH03r2ubp4hPo9BpCcUjDxmpzE18Q6qtnUBfLxMiIyHrEq7VrCtkdB65TOHdAKomLXCFdmpSJEBOmZDi7BCAZ1d4%7EZSxF2Lq4wqFZwNg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048fa81e2fe742ba8cd39f6f1bcd31b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:  48%|####8     | 3.19G/6.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b111a249bd24c848ecdb39a2de0706d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "#Optional: Turn off safety checker\n",
    "pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3cf27-9d5c-4def-a3b2-fad304f6727f",
   "metadata": {},
   "source": [
    "### Define the prompt and generate image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3545edea-8c40-43ef-825d-1f22ed7962b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39749a0385b422ca7b70d20ddae2958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected locale \"C\" with character encoding \"ANSI_X3.4-1968\", which is not UTF-8.\n",
      "Qt depends on a UTF-8 locale, and has switched to \"C.UTF-8\" instead.\n",
      "If this causes problems, reconfigure your locale. See the locale(1) manual\n",
      "for more information.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A robotic Cat on a building\"\n",
    "\n",
    "num_inference_steps = 30\n",
    "\n",
    "generated_image = pipe(prompt, num_inference_steps=num_inference_steps).images[0]\n",
    "generated_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a634e42-d5b0-421f-a046-3a4d85aac53b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
